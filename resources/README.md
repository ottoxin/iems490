# Course Resources

This directory contains supplementary materials and references for IEMS490.

## Essential Papers

### Foundational Papers

1. **Attention Is All You Need** (Vaswani et al., 2017)
   - Original Transformer paper
   - https://arxiv.org/abs/1706.03762

2. **BERT: Pre-training of Deep Bidirectional Transformers** (Devlin et al., 2018)
   - Masked language modeling
   - https://arxiv.org/abs/1810.04805

3. **Language Models are Few-Shot Learners** (Brown et al., 2020)
   - GPT-3 and in-context learning
   - https://arxiv.org/abs/2005.14165

4. **Training language models to follow instructions** (Ouyang et al., 2022)
   - InstructGPT and RLHF
   - https://arxiv.org/abs/2203.02155

### Scaling and Efficiency

5. **Training Compute-Optimal Large Language Models** (Hoffmann et al., 2022)
   - Chinchilla scaling laws
   - https://arxiv.org/abs/2203.15556

6. **LLaMA: Open and Efficient Foundation Language Models** (Touvron et al., 2023)
   - Efficient open-source models
   - https://arxiv.org/abs/2302.13971

7. **FlashAttention: Fast and Memory-Efficient Exact Attention** (Dao et al., 2022)
   - Efficient attention computation
   - https://arxiv.org/abs/2205.14135

### Fine-tuning and Adaptation

8. **LoRA: Low-Rank Adaptation of Large Language Models** (Hu et al., 2021)
   - Parameter-efficient fine-tuning
   - https://arxiv.org/abs/2106.09685

9. **Prefix-Tuning: Optimizing Continuous Prompts** (Li & Liang, 2021)
   - https://arxiv.org/abs/2101.00190

10. **The Power of Scale for Parameter-Efficient Prompt Tuning** (Lester et al., 2021)
    - https://arxiv.org/abs/2104.08691

### Evaluation and Analysis

11. **Measuring Massive Multitask Language Understanding** (Hendrycks et al., 2020)
    - MMLU benchmark
    - https://arxiv.org/abs/2009.03300

12. **Beyond the Imitation Game Benchmark** (Srivastava et al., 2022)
    - BIG-bench
    - https://arxiv.org/abs/2206.04615

### Safety and Alignment

13. **Constitutional AI: Harmlessness from AI Feedback** (Bai et al., 2022)
    - https://arxiv.org/abs/2212.08073

14. **Red Teaming Language Models** (Perez et al., 2022)
    - https://arxiv.org/abs/2202.03286

## Online Resources

### Tutorials and Courses

- **Hugging Face Course**: https://huggingface.co/course
- **Stanford CS224N**: Natural Language Processing with Deep Learning
- **DeepLearning.AI**: Courses on LLMs and prompt engineering
- **The Illustrated Transformer**: https://jalammar.github.io/illustrated-transformer/

### Documentation

- **PyTorch**: https://pytorch.org/docs/
- **Transformers Library**: https://huggingface.co/docs/transformers
- **Tokenizers**: https://huggingface.co/docs/tokenizers

### Datasets

- **The Pile**: Large-scale text dataset
- **C4**: Colossal Clean Crawled Corpus
- **Common Crawl**: Web-scale text data
- **Hugging Face Datasets**: https://huggingface.co/datasets

### Blogs and Communities

- **OpenAI Research**: https://openai.com/research
- **Anthropic Research**: https://www.anthropic.com/research
- **Google AI Blog**: https://ai.googleblog.com/
- **Papers with Code**: https://paperswithcode.com/
- **r/MachineLearning**: Reddit community

## Tools and Libraries

### Core Frameworks
- **PyTorch**: Primary deep learning framework
- **Transformers**: Pre-trained models and utilities
- **Datasets**: Easy access to datasets

### Specialized Tools
- **bitsandbytes**: Quantization and 8-bit optimizers
- **PEFT**: Parameter-efficient fine-tuning methods
- **DeepSpeed**: Large-scale model training
- **Accelerate**: Multi-GPU training utilities
- **wandb**: Experiment tracking

### Development Tools
- **Jupyter**: Interactive notebooks
- **VS Code**: Code editor with great Python support
- **Git**: Version control

## Recommended Textbooks

1. **Speech and Language Processing** (Jurafsky & Martin)
   - Comprehensive NLP textbook
   - Available online: https://web.stanford.edu/~jurafsky/slp3/

2. **Deep Learning** (Goodfellow, Bengio, Courville)
   - Deep learning foundations
   - Available online: https://www.deeplearningbook.org/

3. **Natural Language Processing** (Eisenstein)
   - Modern NLP perspective
   - Available online: https://github.com/jacobeisenstein/gt-nlp-class

## Additional Topics

### Advanced Architectures
- Mixture of Experts (MoE)
- State Space Models (SSMs)
- Retrieval-augmented models

### Applications
- Code generation
- Scientific text understanding
- Multimodal reasoning
- Agent systems

### Research Frontiers
- Long-context models
- Efficient architectures
- Controllable generation
- Reasoning capabilities

## Getting Help

- **Office Hours**: Check course schedule
- **Discussion Forum**: Course management system
- **Stack Overflow**: For technical questions
- **GitHub Issues**: For code-related problems

## Contributing

If you find useful resources not listed here, please submit a pull request or open an issue!
