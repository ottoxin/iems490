# IEMS490: Theory and Algorithms for Large Language Models

## Course Information

**Semester:** [To be filled]  
**Meeting Time:** [To be filled]  
**Location:** [To be filled]  
**Instructor:** [To be filled]  
**Office Hours:** [To be filled]  
**Email:** [To be filled]  

## Course Description

This course provides a comprehensive introduction to the theoretical foundations and algorithmic techniques underlying modern Large Language Models (LLMs). Students will learn about transformer architectures, pre-training methodologies, fine-tuning techniques, and practical applications of LLMs. The course combines theoretical understanding with hands-on implementation experience.

## Learning Objectives

By the end of this course, students will be able to:

1. Understand the architecture and mechanics of transformer-based language models
2. Implement key components of LLMs from scratch
3. Train and fine-tune language models for various tasks
4. Apply prompt engineering techniques for effective LLM usage
5. Evaluate LLM performance using appropriate metrics and benchmarks
6. Understand scaling laws and efficiency considerations
7. Recognize ethical implications and safety concerns
8. Stay current with rapidly evolving LLM research

## Prerequisites

- **Required:**
  - Strong programming skills (Python)
  - Linear algebra and calculus
  - Probability and statistics
  - Machine learning fundamentals

- **Recommended:**
  - Deep learning experience
  - Natural language processing background
  - Prior experience with PyTorch or TensorFlow

## Course Materials

### Textbooks
- **Primary:** Course lecture notes and selected research papers
- **Supplementary:** 
  - Speech and Language Processing (Jurafsky & Martin)
  - Deep Learning (Goodfellow, Bengio, Courville)

### Software Requirements
- Python 3.8+
- PyTorch
- Hugging Face Transformers
- Jupyter Notebook
- See `requirements.txt` for complete list

### Hardware Requirements
- Access to GPU resources (provided through course or cloud services)
- Minimum 16GB RAM recommended for local development

## Course Schedule

### Module 1: Foundations (Weeks 1-3)
- Week 1: Introduction to Language Models
- Week 2: Transformer Architecture and Attention
- Week 3: Tokenization and Embeddings

### Module 2: Training and Pre-training (Weeks 4-6)
- Week 4: Pre-training Objectives and Strategies
- Week 5: Optimization and Training Dynamics
- Week 6: Scaling Laws and Compute Efficiency

### Module 3: Fine-tuning and Adaptation (Weeks 7-9)
- Week 7: Fine-tuning Methods
- Week 8: Parameter-Efficient Fine-tuning (PEFT)
- Week 9: Prompt Engineering and In-Context Learning

### Module 4: Advanced Topics (Weeks 10-12)
- Week 10: Evaluation and Benchmarking
- Week 11: Alignment, Safety, and Ethics
- Week 12: Current Research and Future Directions

## Assignments and Grading

### Grade Breakdown

| Component | Weight | Description |
|-----------|--------|-------------|
| Assignments | 40% | 5 programming assignments |
| Midterm Exam | 15% | In-class or take-home |
| Final Project | 30% | Team project with presentation |
| Participation | 10% | Class and online participation |
| Paper Presentations | 5% | Present recent research papers |

### Assignment Schedule

1. **Assignment 1:** Tokenization and Embeddings (Week 3)
2. **Assignment 2:** Attention Mechanisms (Week 4)
3. **Assignment 3:** Training a Small LM (Week 6)
4. **Assignment 4:** Fine-tuning and PEFT (Week 8)
5. **Assignment 5:** Prompt Engineering (Week 10)

### Project Timeline

- **Week 3:** Form teams, submit proposal
- **Week 7:** Progress report due
- **Week 11:** Project presentations
- **Week 12:** Final report and code submission

### Grading Scale

- A: 93-100%
- A-: 90-92%
- B+: 87-89%
- B: 83-86%
- B-: 80-82%
- C+: 77-79%
- C: 73-76%
- C-: 70-72%
- D: 60-69%
- F: Below 60%

## Course Policies

### Attendance
Regular attendance is expected. More than 3 unexcused absences may affect your participation grade.

### Late Submission Policy
- Assignments submitted within 24 hours: 10% penalty
- Assignments submitted within 48 hours: 25% penalty
- Assignments submitted after 48 hours: 50% penalty
- No submissions accepted after 72 hours without prior arrangement

### Academic Integrity
All work must be your own. You may discuss concepts with classmates, but all code and written work must be produced individually unless explicitly stated as group work. Violations will be reported to the university.

### Collaboration Policy
- Assignments: Individual work, but conceptual discussions allowed
- Project: Collaborative within teams
- Exams: Individual work only

### Use of AI Tools
Given the nature of this course, you may use LLMs as learning aids, but:
- All submitted work must be your own understanding
- Disclose any AI assistance used
- Focus on learning, not just getting answers
- Will be tested on understanding during exams

### Accessibility
Students with disabilities requiring accommodations should contact the instructor and the university's disability services office.

## Communication

### Course Website
All materials, announcements, and assignments will be posted on:
- GitHub repository: https://github.com/ottoxin/iems490
- [Course management system]

### Discussion Forum
Use the course forum for:
- Technical questions
- Clarifications on assignments
- Discussion of course materials
- Study group formation

### Office Hours
- In-person and virtual options available
- Schedule posted on course website
- Additional appointments by arrangement

## Computing Resources

### GPU Access
Students will have access to:
- University computing cluster
- Cloud computing credits (Google Colab Pro, AWS, etc.)
- Instructions provided in first week

### Software Support
- Installation guides in repository
- Technical support during office hours
- Peer support through course forum

## Additional Resources

### Research Papers
- Weekly assigned readings from recent conferences (NeurIPS, ICML, ACL, etc.)
- Papers available through university library or arXiv

### Online Resources
- Hugging Face documentation and tutorials
- Stanford CS224N materials
- OpenAI and Anthropic research blogs

### Optional Enrichment
- Guest lectures from industry practitioners
- Research seminars
- Hackathons and competitions

## Important Dates

- **Week 1:** Course introduction, environment setup
- **Week 3:** Assignment 1 due, Project proposals due
- **Week 6:** Assignment 3 due, Midterm exam
- **Week 7:** Project progress reports due
- **Week 10:** Assignment 5 due
- **Week 11:** Project presentations
- **Week 12:** Final project reports due

## Learning Outcomes Assessment

Student learning will be assessed through:
1. **Technical Skills:** Programming assignments and project
2. **Theoretical Understanding:** Exams and written responses
3. **Research Abilities:** Paper presentations and project
4. **Communication:** Presentations and documentation
5. **Critical Thinking:** Project design and analysis

## Course Improvement

Your feedback is valuable! Please provide feedback through:
- Mid-semester survey
- End-of-course evaluations
- Direct communication with instructor
- Anonymous feedback form

## Disclaimer

This syllabus is subject to change. Any changes will be announced in class and posted on the course website. Students are responsible for staying informed of updates.

---

**Last Updated:** [Date]  
**Version:** 1.0
