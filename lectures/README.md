# Lecture Notes

This directory contains lecture notes and slides for IEMS490.

## Course Schedule

### Week 1: Introduction to LLMs
- Overview of language models
- Historical context: from n-grams to transformers
- Key applications and use cases

### Week 2: Transformer Architecture
- Attention mechanisms
- Multi-head attention
- Positional encoding
- Architecture details

### Week 3: Pre-training Objectives
- Masked language modeling
- Causal language modeling
- Next sentence prediction
- Training dynamics

### Week 4: Tokenization
- Byte-pair encoding (BPE)
- WordPiece and SentencePiece
- Tokenization strategies
- Vocabulary construction

### Week 5: Optimization and Training
- Adam and its variants
- Learning rate schedules
- Gradient accumulation
- Mixed precision training

### Week 6: Fine-tuning Methods
- Full fine-tuning
- Parameter-efficient fine-tuning (PEFT)
- LoRA and QLoRA
- Adapter methods

### Week 7: Prompt Engineering
- Zero-shot and few-shot learning
- Chain-of-thought prompting
- Instruction tuning
- Prompt design strategies

### Week 8: Evaluation and Benchmarks
- Perplexity and likelihood
- GLUE and SuperGLUE
- Task-specific metrics
- Human evaluation

### Week 9: Scaling Laws
- Chinchilla scaling laws
- Compute-optimal training
- Model size vs. data size
- Emergent abilities

### Week 10: Efficient Inference
- Model quantization
- Pruning and distillation
- KV cache optimization
- Speculative decoding

### Week 11: Alignment and Safety
- Reinforcement learning from human feedback (RLHF)
- Constitutional AI
- Safety considerations
- Bias and fairness

### Week 12: Advanced Topics
- Retrieval-augmented generation (RAG)
- Multi-modal models
- Future directions
- Research opportunities

## Lecture Materials

Each week's materials will include:
- PDF slides
- Jupyter notebooks with code examples
- Reading assignments
- Discussion questions
